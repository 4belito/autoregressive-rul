{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9feaeeb4-402e-4d4a-afc9-67409e4d8066",
   "metadata": {},
   "source": [
    "# Overall Model Training\n",
    "\n",
    "Train and evaluate the model on the whole system(rectifier) and each capacitor(isolated or on the system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6535e3-9a35-4d65-bffa-4cca31a0c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('./data')\n",
    "from datautil import create_loaders\n",
    "from architectures import RUL_transition,RUL_estimation,AutoencoderMLP\n",
    "from training_decoder import train_estimation,train_transition,train_autoencoder \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10c64d39-ae54-47f2-893c-80fd04e89487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4c0f6-de84-4a61-9af0-1f432ab939e7",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b485d38-8988-4f24-b4ed-d54d9f5c61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "data='Rectifier'#'-Rectifier_300Diode'\n",
    "dataset_name='Rectifier'#'RectifierRUL'# \n",
    "exp_est='estimation_noisy0'#'RUL_estimation3'# \n",
    "exp_tra='transition3'#'prueba'#\n",
    "dim=3\n",
    "exp_aut=f'autoencoder_dim{dim}'\n",
    "exp=f'overall{dim}'#'overall_noisy' #'overall3'\n",
    "\n",
    "\n",
    "#### choose the component(if Rectifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b41f32c-2a40-4619-af75-14e3d34fe5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_address=f'./Experiments/{data}/{dataset_name}_{exp}'\n",
    "exp_address_tra=f'./Experiments/{data}/{dataset_name}_{exp_tra}'\n",
    "exp_address_est=f'./Experiments/{data}/{dataset_name}_{exp_est}'\n",
    "exp_address_aut=f'./Experiments/{data}/{dataset_name}_{exp_aut}'\n",
    "#extended='' #'_ext'#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cfa79-06d7-4ba6-98be-76b9c771c153",
   "metadata": {},
   "source": [
    "# Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cf69e29-9c32-486a-818b-bed6c9b7bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name_aut = exp_address_aut+'/estudio'\n",
    "study_name_tra = exp_address_tra+'/estudio'\n",
    "study_name_est = exp_address_est+'/estudio'\n",
    "storage_name_tra = \"sqlite:///{}.db\".format(study_name_tra)\n",
    "storage_name_est = \"sqlite:///{}.db\".format(study_name_est)\n",
    "storage_name_aut = \"sqlite:///{}.db\".format(study_name_aut)\n",
    "study_tra = optuna.load_study(study_name=study_name_tra, storage=storage_name_tra)\n",
    "study_est = optuna.load_study(study_name=study_name_est, storage=storage_name_est)\n",
    "study_aut = optuna.load_study(study_name=study_name_aut, storage=storage_name_aut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e10c1e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer3': 200,\n",
       " 'layer2': 240,\n",
       " 'layer1': 120,\n",
       " 'emb_dim': 3,\n",
       " 'n_epoch': 47,\n",
       " 'lr': 0.0009128718305268314}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_aut.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c471f18d-488a-4eec-a19b-f9879c90dc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rul_head_dim': 64,\n",
       " 'n_head': 11,\n",
       " 'embed_dim/n_head': 6,\n",
       " 'dim_feedforward': 128,\n",
       " 'N_epoch': 25,\n",
       " 'lr': 8.382056903708147e-05}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_tra.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ca67915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rul_head_dim': 32,\n",
       " 'n_head': 9,\n",
       " 'embed_dim/n_head': 2,\n",
       " 'dim_feedforward': 64,\n",
       " 'N_epoch': 30,\n",
       " 'noise_strength_ini': 7.475274613924334,\n",
       " 'noise_strength_end': 5.605270196679893,\n",
       " 'noise_bound_ini': 1.0496990752839725,\n",
       " 'noise_bound_end': 0.5570464204713121,\n",
       " 'lr': 0.0004231768025787699}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_est.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b444b18-52ad-43bb-a51e-7394f53953f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder trial numb: 76\n",
      "Autoencoder best trial: 0.4418268822639031\n",
      "Transition trial numb: 100\n",
      "Transition best trial: 0.02298082523611841\n",
      "Estimation trial numb: 15\n",
      "Estimation best trial: 10.858512707796121\n"
     ]
    }
   ],
   "source": [
    "print(f'Autoencoder trial numb: {len(study_aut.trials)}')\n",
    "print(f'Autoencoder best trial: {study_aut.best_trial.value}')\n",
    "print(f'Transition trial numb: {len(study_tra.trials)}')\n",
    "print(f'Transition best trial: {study_tra.best_trial.value}')\n",
    "print(f'Estimation trial numb: {len(study_est.trials)}')\n",
    "print(f'Estimation best trial: {study_est.best_trial.value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276d10c-8d42-4d1e-8232-257b24712300",
   "metadata": {},
   "source": [
    "### Create Loaders and train models using optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc2d4ef5-dd6a-4b2d-864b-32dd99eede4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss='MSE'\n",
    "weighted=False\n",
    "\n",
    "\n",
    "train_config_aut={'n_epoch':study_est.best_trial.params['N_epoch'],#500\n",
    "            'lr':  study_est.best_trial.params['lr']}\n",
    "\n",
    "\n",
    "\n",
    "train_config_tra={'n_epoch':study_tra.best_trial.params['N_epoch'],#500\n",
    "            'lr':  study_tra.best_trial.params['lr'],#1e-4\n",
    "            'a':10,#1 10\n",
    "            'b':13, #20 13\n",
    "            'alpha':1,\n",
    "            'war':200,\n",
    "            'Floss':loss,\n",
    "            'weighted':weighted}\n",
    "\n",
    "train_config_est={'n_epoch':study_est.best_trial.params['N_epoch'],#500\n",
    "            'lr':  study_est.best_trial.params['lr'],#1e-4\n",
    "            'a':10,#1 10\n",
    "            'b':13, #20 13\n",
    "            'alpha':1,\n",
    "            'war':200,\n",
    "            'Floss':loss,\n",
    "            'weighted':weighted}\n",
    "\n",
    "\n",
    "\n",
    "with open(f'./data/{data}/dataset.pkl', 'rb') as file:\n",
    "    train_dataset = pickle.load(file)\n",
    "    \n",
    "with open(f'./data/{data}/dataset_Test.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)\n",
    "\n",
    "window=30 \n",
    "batch_size=64\n",
    "noisy=True\n",
    "train_loader, test_loader=create_loaders(train_dataset=train_dataset,test_dataset=test_dataset,window=window+1,batch_size=(batch_size,256),scored=noisy,stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3fdb5",
   "metadata": {},
   "source": [
    "#### Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a845ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_aut=study_aut.best_trial.params\n",
    "config_aut['n_features']=train_dataset.n_features\n",
    "config_aut['window']=window\n",
    "config_tra=study_tra.best_trial.params\n",
    "config_tra['n_features']=train_dataset.n_features\n",
    "config_tra['window']=window\n",
    "config_est=study_est.best_trial.params\n",
    "config_est['n_features']=train_dataset.n_features\n",
    "config_est['window']=window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af03ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if noisy:\n",
    "    noise_strength_ini=study_est.best_trial.params['noise_strength_ini']  \n",
    "    noise_strength_end=study_est.best_trial.params['noise_strength_end' ]  \n",
    "    noise_bound_ini=study_est.best_trial.params['noise_bound_ini']    \n",
    "    noise_bound_end=study_est.best_trial.params['noise_bound_end']    \n",
    "    noise=(noise_strength_ini,noise_bound_ini,noise_strength_end,noise_bound_end) \n",
    "else:\n",
    "    noise=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8546252e-e6c7-4022-be07-00e8fe322164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study.best_trial.params['look_back']    \n",
    "\n",
    "model_aut=AutoencoderMLP(dim=[config_aut['window'],config_aut['n_features']],\n",
    "                layers=[config_aut['layer1'],config_aut['layer2'],config_aut['layer3']],\n",
    "                emb_dim=config_aut['emb_dim']).to(device).double()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_tra=RUL_transition(look_back=config_tra['window'],\n",
    "                                        n_features=config_tra['n_features'],\n",
    "                                        embed_dim=config_tra['embed_dim/n_head']*config_tra['n_head'],\n",
    "                                        rul_head_dim=config_tra['rul_head_dim'],\n",
    "                                        dim_feedforward=config_tra['dim_feedforward'],\n",
    "                                        n_head=config_tra['n_head']\n",
    "                                        ).to(device).double()\n",
    "\n",
    "model_est=RUL_estimation(look_back=config_est['window'],\n",
    "                                        n_features=config_est['n_features'],\n",
    "                                        embed_dim=config_est['embed_dim/n_head']*config_est['n_head'],\n",
    "                                        rul_head_dim=config_est['rul_head_dim'],\n",
    "                                        dim_feedforward=config_est['dim_feedforward'],\n",
    "                                        n_head=config_est['n_head']\n",
    "                                        ).to(device).double()\n",
    "\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "one_class_svm=OneClassSVM(nu=0.0001,kernel='rbf',gamma=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba396b70-da5e-41b7-81df-95e596fbb162",
   "metadata": {},
   "source": [
    "## Train estimation and transition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cee8a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=train_loader.dataset.x[:,:-1,:]\n",
    "# X_train=X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "# one_class_svm_trained=one_class_svm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76651ac-16b0-4488-a6d0-7f4a9fe2e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model_aut_trained=train_autoencoder(model_aut,train_loader,config_aut)\n",
    "print('done')\n",
    "#model_est_trained=train_estimation(model_est,train_loader,train_config_est,noise_coef=noise)\n",
    "print('done')\n",
    "#model_tra_trained=train_transition(model_tra,train_loader,train_config_tra)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df413760",
   "metadata": {},
   "source": [
    "## Save dataset and learned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "238d59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(exp_address, exist_ok=True)\n",
    "torch.save(train_loader, f'{exp_address}/train_loader.pkl')\n",
    "torch.save(test_loader, f'{exp_address}/test_loader.pkl')\n",
    "\n",
    "\n",
    "# with open(f'{exp_address}/model_one_class_svm.pkl','wb') as f:\n",
    "#     pickle.dump(one_class_svm,f)\n",
    "\n",
    "    \n",
    "weights_aut=model_aut_trained.state_dict()\n",
    "#weights_tra=model_tra_trained.state_dict()\n",
    "#weights_est=model_est_trained.state_dict()\n",
    "torch.save(weights_aut, f'{exp_address}/model_aut.pt')\n",
    "#torch.save(weights_tra, f'{exp_address}/model_tra.pt')\n",
    "#torch.save(weights_est, f'{exp_address}/model_est.pt')\n",
    "torch.save(config_aut, f'{exp_address}/config_aut.pkl')\n",
    "torch.save(config_est, f'{exp_address}/config_est.pkl')\n",
    "torch.save(config_tra, f'{exp_address}/config_tra.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7902c",
   "metadata": {},
   "source": [
    "## Load learned models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a066915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.load(f'{exp_address}/train_loader.pkl')\n",
    "test_loader=torch.load(f'{exp_address}/test_loader.pkl')\n",
    "\n",
    "\n",
    "# with open(f'{exp_address}/model_one_class_svm.pkl', 'rb') as f:\n",
    "#     one_class_svm = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "weights_aut=torch.load(f'{exp_address}/model_aut.pt')\n",
    "weights_tra=torch.load(f'{exp_address}/model_tra.pt')\n",
    "weights_est=torch.load(f'{exp_address}/model_est.pt')\n",
    "\n",
    "config_aut=torch.load(f'{exp_address}/config_aut.pkl')\n",
    "config_est=torch.load(f'{exp_address}/config_est.pkl')\n",
    "config_tra=torch.load(f'{exp_address}/config_tra.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c0a4988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Experiments/Rectifier/Rectifier_overall1/model_aut.pt'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{exp_address}/model_aut.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50c434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f46642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02b04582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=train_loader.dataset.x[:,:-1,:]\n",
    "#X_train=X_train.reshape(X_train.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8d32ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45200, 30, 9)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba167b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flatten() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abeldg/CODE/Transformer code/Training-Overall_model.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.2.219.98/home/abeldg/CODE/Transformer%20code/Training-Overall_model.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_aut_trained(X_train)\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CODE/Transformer code/architectures.py:301\u001b[0m, in \u001b[0;36mAutoencoderMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 301\u001b[0m     z\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z)\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CODE/Transformer code/architectures.py:258\u001b[0m, in \u001b[0;36mEncoderMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/abel/lib/python3.11/site-packages/torch/nn/modules/flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: flatten() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "model_aut_trained(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "855bdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "one_class_svm=OneClassSVM(nu=0.1,kernel='rbf',gamma='auto')\n",
    "with open(f'{exp_address}/model_one_class_svm.pkl','wb') as f:\n",
    "    pickle.dump(one_class_svm,f)\n",
    "X_train=train_loader.dataset.x[:,:-1,:]\n",
    "X_train=X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "\n",
    "one_class_svm_trained=one_class_svm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8f198e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test_loader.dataset.x[:,:-1,:]\n",
    "X_test=X_test.reshape(X_test.shape[0],-1)\n",
    "prediction=one_class_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1e3eb720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10950,)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "41ab9b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7884"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "160dc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_false=torch.randn_like(torch.tensor(X_test))\n",
    "X_false=X_false.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "75ad426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=one_class_svm.predict(X_test+0.2*X_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "55682cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6094"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
